{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os \n",
    "import data\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import Model\n",
    "import random\n",
    "\n",
    "\n",
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    total_samples = total_batches * batch_size\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        vectors = vectors.transpose(1, 0)\n",
    "        \n",
    "        labels = Variable(torch.stack(labels).squeeze().type('torch.FloatTensor')) \n",
    "        vectors = Variable(vectors)\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(vectors, hidden)\n",
    "        lossy = loss_(output, labels)\n",
    "        epoch_loss += lossy.data[0] * batch_size\n",
    "\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            loss_train = evaluate(model, train_eval_iter,batch_size)\n",
    "            loss_dev = evaluate(model, dev_iter,batch_size)\n",
    "            kappa_dev = evaluate_kappa(model, dev_iter,batch_size)\n",
    "            with open(\"test.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"Epoch %i; Step %i; Avg Loss %f; Train loss: %f; Dev loss: %f; Dev kappa: %f\\n\" \n",
    "                  %(epoch, step, epoch_loss/total_samples, loss_train, loss_dev, kappa_dev))\n",
    "            print(\"Epoch %i; Step %i; Avg Loss %f; Train loss: %f; Dev loss: %f; Dev kappa: %f\" \n",
    "                  %(epoch, step, epoch_loss/total_samples, loss_train, loss_dev, kappa_dev))\n",
    "            epoch += 1\n",
    "            \n",
    "        if step % 5 == 0:\n",
    "            with open(\"test.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"Epoch %i; Step %i; loss %f\\n\" %(epoch, step, lossy.data[0]))\n",
    "            print(\"Epoch %i; Step %i; loss %f\" %(epoch, step, lossy.data[0]))\n",
    "        step += 1\n",
    "\n",
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter, batch_size):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    evalloss = 0.0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        vectors = vectors.transpose(1, 0)\n",
    "        \n",
    "        labels = Variable(torch.stack(labels).squeeze().type('torch.FloatTensor'))\n",
    "        vectors = Variable(vectors)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(vectors, hidden)\n",
    "        evalloss += F.mse_loss(output, labels).data[0]\n",
    "    return evalloss/len(data_iter)\n",
    "\n",
    "\n",
    "def evaluate_kappa(model, data_iter, batch_size):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        vectors = vectors.transpose(1, 0)\n",
    "\n",
    "        vectors = Variable(vectors)\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(vectors, hidden)\n",
    "\n",
    "        predicted = [int(round(float(num))) for num in output.data.cpu().numpy()]\n",
    "        predicted_labels.extend([round(float(num)) for num in output.data.cpu().numpy()])\n",
    "        labels = [int(label[0]) for label in labels]\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "    return cohen_kappa_score(true_labels, predicted_labels, weights = \"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0.2, bidirectional = False, pre_emb=None):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = getattr(nn, rnn_type)(embedding_dim, hidden_size, num_layers, bias=False, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.decoder = nn.Linear(hidden_size, 1)\n",
    "        self.decoder_bi = nn.Linear(hidden_size*2, 1)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self, pre_emb=None):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.encoder(inputs)\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        # mot here\n",
    "        output = torch.mean(output, 0)\n",
    "        output = torch.squeeze(output)\n",
    "        print(output.size())\n",
    "        if self.bidirectional:\n",
    "            decoded = self.decoder_bi(output)\n",
    "        else:\n",
    "            decoded = self.decoder(output)\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ##next(rnn.parameters()).data=rnn.encoder.weight.data\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.bidirectional == True:\n",
    "            return (Variable(weight.new(self.num_layers * 2, batch_size, self.hidden_size).zero_()),\n",
    "                    Variable(weight.new(self.num_layers * 2, batch_size, self.hidden_size).zero_()))\n",
    "        else:\n",
    "            return (Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_()),\n",
    "                    Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_()))            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, labels = get_batch(next(training_iter)) \n",
    "vectors = torch.stack(vectors).squeeze()\n",
    "vectors = vectors.transpose(1, 0)\n",
    "labels = Variable(torch.stack(labels).squeeze().type('torch.FloatTensor')) \n",
    "vectors = Variable(vectors)\n",
    "emb = encoder(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = next(rnn.parameters()).data\n",
    "hidden = (Variable(weight.new(num_layers, batch_size, hidden_size).zero_()),\n",
    "                    Variable(weight.new(num_layers, batch_size, hidden_size).zero_()))            \n",
    "#hidden:(1x20x24,1x20x24)\n",
    "rnn = nn.LSTM(embedding_dim, hidden_size, num_layer, bias=False, dropout=0.5, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-a7fc5f225c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rnn(emb, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading!\n",
      "max seq length:  1064\n",
      "data_size 12977\n",
      "Finished Converting!\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"../data/training_final.csv\", sep=',',header=0, index_col=0)\n",
    "data_set = data.get_data(raw_data)\n",
    "print('Finished Loading!')\n",
    "\n",
    "#get max sequence length\n",
    "max_seq_length = max(list(map(lambda x:len(x.split()),raw_data.essay)))\n",
    "print('max seq length: ', max_seq_length)\n",
    "\n",
    "# split to train/val/test\n",
    "data_size = len(data_set)\n",
    "print('data_size',data_size)\n",
    "training_set = data_set[:int(data_size*0.8)]\n",
    "dev_set = data_set[int(data_size*0.8):int(data_size*0.9)]\n",
    "test_set = data_set[int(data_size*0.9):]\n",
    "\n",
    "\n",
    "# convert and formatting\n",
    "word_to_ix, vocab_size = data.build_dictionary([training_set])\n",
    "#print('vocab size', vocab_size)\n",
    "data.sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set], max_seq_length)\n",
    "print('Finished Converting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training:\n",
      "Epoch 0; Step 0; Avg Loss 0.265792; Train loss: 112.579555; Dev loss: 113.170442; Dev kappa: -0.027539\n",
      "Epoch 1; Step 0; loss 137.946243\n",
      "Epoch 1; Step 5; loss 41.789780\n",
      "Epoch 1; Step 10; loss 80.356880\n",
      "Epoch 1; Step 15; loss 104.155869\n",
      "Epoch 1; Step 20; loss 47.958649\n",
      "Epoch 1; Step 25; loss 24.629101\n",
      "Epoch 1; Step 30; loss 253.317505\n",
      "Epoch 1; Step 35; loss 117.176674\n",
      "Epoch 1; Step 40; loss 107.530396\n",
      "Epoch 1; Step 45; loss 88.053818\n",
      "Epoch 1; Step 50; loss 77.529739\n",
      "Epoch 1; Step 55; loss 33.404148\n",
      "Epoch 1; Step 60; loss 42.197281\n",
      "Epoch 1; Step 65; loss 46.120537\n",
      "Epoch 1; Step 70; loss 60.361961\n",
      "Epoch 1; Step 75; loss 27.875690\n",
      "Epoch 1; Step 80; loss 13.794611\n",
      "Epoch 1; Step 85; loss 24.388214\n",
      "Epoch 1; Step 90; loss 24.260725\n",
      "Epoch 1; Step 95; loss 14.790932\n",
      "Epoch 1; Step 100; loss 12.569665\n",
      "Epoch 1; Step 105; loss 5.861126\n",
      "Epoch 1; Step 110; loss 10.826163\n",
      "Epoch 1; Step 115; loss 7.249639\n",
      "Epoch 1; Step 120; loss 14.581629\n",
      "Epoch 1; Step 125; loss 65.000916\n",
      "Epoch 1; Step 130; loss 29.460361\n",
      "Epoch 1; Step 135; loss 9.806449\n",
      "Epoch 1; Step 140; loss 10.509560\n",
      "Epoch 1; Step 145; loss 37.859589\n",
      "Epoch 1; Step 150; loss 10.871263\n",
      "Epoch 1; Step 155; loss 16.078705\n",
      "Epoch 1; Step 160; loss 14.941778\n",
      "Epoch 1; Step 165; loss 18.892475\n",
      "Epoch 1; Step 170; loss 10.755197\n",
      "Epoch 1; Step 175; loss 26.107555\n",
      "Epoch 1; Step 180; loss 43.994377\n",
      "Epoch 1; Step 185; loss 4.100404\n",
      "Epoch 1; Step 190; loss 58.927223\n",
      "Epoch 1; Step 195; loss 14.934588\n",
      "Epoch 1; Step 200; loss 11.711668\n",
      "Epoch 1; Step 205; loss 10.756971\n",
      "Epoch 1; Step 210; loss 41.300671\n",
      "Epoch 1; Step 215; loss 25.226025\n",
      "Epoch 1; Step 220; loss 8.701429\n",
      "Epoch 1; Step 225; loss 8.210802\n",
      "Epoch 1; Step 230; loss 14.810839\n",
      "Epoch 1; Step 235; loss 10.283408\n",
      "Epoch 1; Step 240; loss 19.967503\n",
      "Epoch 1; Step 245; loss 17.796139\n",
      "Epoch 1; Step 250; loss 5.109717\n",
      "Epoch 1; Step 255; loss 7.570960\n",
      "Epoch 1; Step 260; loss 9.047911\n",
      "Epoch 1; Step 265; loss 18.613556\n",
      "Epoch 1; Step 270; loss 28.176565\n",
      "Epoch 1; Step 275; loss 26.927505\n",
      "Epoch 1; Step 280; loss 21.425190\n",
      "Epoch 1; Step 285; loss 13.175957\n",
      "Epoch 1; Step 290; loss 38.812084\n",
      "Epoch 1; Step 295; loss 8.312555\n",
      "Epoch 1; Step 300; loss 51.528973\n",
      "Epoch 1; Step 305; loss 11.084085\n",
      "Epoch 1; Step 310; loss 14.017827\n",
      "Epoch 1; Step 315; loss 10.038176\n",
      "Epoch 1; Step 320; loss 32.669945\n",
      "Epoch 1; Step 325; loss 11.249715\n",
      "Epoch 1; Step 330; loss 6.664297\n",
      "Epoch 1; Step 335; loss 40.838520\n",
      "Epoch 1; Step 340; loss 17.485722\n",
      "Epoch 1; Step 345; loss 25.723797\n",
      "Epoch 1; Step 350; loss 4.727489\n",
      "Epoch 1; Step 355; loss 9.313671\n",
      "Epoch 1; Step 360; loss 5.484220\n",
      "Epoch 1; Step 365; loss 5.923039\n",
      "Epoch 1; Step 370; loss 13.273733\n",
      "Epoch 1; Step 375; loss 17.918571\n",
      "Epoch 1; Step 380; loss 11.527307\n",
      "Epoch 1; Step 385; loss 23.053083\n",
      "Epoch 1; Step 390; loss 5.433969\n",
      "Epoch 1; Step 395; loss 7.551072\n",
      "Epoch 1; Step 400; loss 9.483027\n",
      "Epoch 1; Step 405; loss 12.956888\n",
      "Epoch 1; Step 410; loss 26.428110\n",
      "Epoch 1; Step 415; loss 21.496334\n",
      "Epoch 1; Step 420; loss 15.547229\n",
      "Epoch 1; Step 425; loss 19.067692\n",
      "Epoch 1; Step 430; loss 7.413259\n",
      "Epoch 1; Step 435; loss 20.970325\n",
      "Epoch 1; Step 440; loss 40.999111\n",
      "Epoch 1; Step 445; loss 11.373857\n",
      "Epoch 1; Step 450; loss 12.776096\n",
      "Epoch 1; Step 455; loss 2.734427\n",
      "Epoch 1; Step 460; loss 9.652847\n",
      "Epoch 1; Step 465; loss 11.097040\n",
      "Epoch 1; Step 470; loss 4.175544\n",
      "Epoch 1; Step 475; loss 3.175209\n",
      "Epoch 1; Step 480; loss 24.217070\n",
      "Epoch 1; Step 485; loss 28.960215\n",
      "Epoch 1; Step 490; loss 3.452955\n",
      "Epoch 1; Step 495; loss 8.437907\n",
      "Epoch 1; Step 500; loss 10.010449\n",
      "Epoch 1; Step 505; loss 37.576904\n",
      "Epoch 1; Step 510; loss 10.431452\n",
      "Epoch 1; Step 515; loss 9.085286\n",
      "Epoch 1; Step 519; Avg Loss 0.019980; Train loss: 11.351848; Dev loss: 14.065943; Dev kappa: 0.898730\n",
      "Epoch 2; Step 520; loss 2.170825\n",
      "Epoch 2; Step 525; loss 18.335100\n",
      "Epoch 2; Step 530; loss 10.528158\n",
      "Epoch 2; Step 535; loss 12.897412\n",
      "Epoch 2; Step 540; loss 6.451653\n",
      "Epoch 2; Step 545; loss 8.225282\n",
      "Epoch 2; Step 550; loss 15.439998\n",
      "Epoch 2; Step 555; loss 32.112740\n",
      "Epoch 2; Step 560; loss 6.877299\n",
      "Epoch 2; Step 565; loss 6.283371\n",
      "Epoch 2; Step 570; loss 4.511577\n",
      "Epoch 2; Step 575; loss 3.609557\n",
      "Epoch 2; Step 580; loss 8.449809\n",
      "Epoch 2; Step 585; loss 12.217978\n",
      "Epoch 2; Step 590; loss 7.446438\n",
      "Epoch 2; Step 595; loss 4.124272\n",
      "Epoch 2; Step 600; loss 7.578984\n",
      "Epoch 2; Step 605; loss 7.581712\n",
      "Epoch 2; Step 610; loss 3.129432\n",
      "Epoch 2; Step 615; loss 4.199450\n",
      "Epoch 2; Step 620; loss 3.434943\n",
      "Epoch 2; Step 625; loss 8.358216\n",
      "Epoch 2; Step 630; loss 8.588089\n",
      "Epoch 2; Step 635; loss 2.855095\n",
      "Epoch 2; Step 640; loss 2.115380\n",
      "Epoch 2; Step 645; loss 4.958748\n",
      "Epoch 2; Step 650; loss 8.516630\n",
      "Epoch 2; Step 655; loss 19.245474\n",
      "Epoch 2; Step 660; loss 43.666027\n",
      "Epoch 2; Step 665; loss 5.209442\n",
      "Epoch 2; Step 670; loss 4.536267\n",
      "Epoch 2; Step 675; loss 4.775437\n",
      "Epoch 2; Step 680; loss 23.591114\n",
      "Epoch 2; Step 685; loss 21.656103\n",
      "Epoch 2; Step 690; loss 12.512388\n",
      "Epoch 2; Step 695; loss 6.651910\n",
      "Epoch 2; Step 700; loss 6.466085\n",
      "Epoch 2; Step 705; loss 4.341448\n",
      "Epoch 2; Step 710; loss 7.259402\n",
      "Epoch 2; Step 715; loss 11.413764\n",
      "Epoch 2; Step 720; loss 15.688100\n",
      "Epoch 2; Step 725; loss 4.578217\n",
      "Epoch 2; Step 730; loss 46.733761\n",
      "Epoch 2; Step 735; loss 3.687639\n",
      "Epoch 2; Step 740; loss 12.809995\n",
      "Epoch 2; Step 745; loss 6.884618\n",
      "Epoch 2; Step 750; loss 12.668636\n",
      "Epoch 2; Step 755; loss 7.385942\n",
      "Epoch 2; Step 760; loss 10.178272\n",
      "Epoch 2; Step 765; loss 6.592710\n",
      "Epoch 2; Step 770; loss 9.959149\n",
      "Epoch 2; Step 775; loss 13.945874\n",
      "Epoch 2; Step 780; loss 8.824832\n",
      "Epoch 2; Step 785; loss 21.242514\n",
      "Epoch 2; Step 790; loss 12.213816\n",
      "Epoch 2; Step 795; loss 12.329311\n",
      "Epoch 2; Step 800; loss 7.389847\n",
      "Epoch 2; Step 805; loss 7.802699\n",
      "Epoch 2; Step 810; loss 9.754981\n",
      "Epoch 2; Step 815; loss 12.180182\n",
      "Epoch 2; Step 820; loss 4.141086\n",
      "Epoch 2; Step 825; loss 4.166616\n",
      "Epoch 2; Step 830; loss 9.884954\n",
      "Epoch 2; Step 835; loss 3.420779\n",
      "Epoch 2; Step 840; loss 13.169703\n",
      "Epoch 2; Step 845; loss 6.249727\n",
      "Epoch 2; Step 850; loss 5.072314\n",
      "Epoch 2; Step 855; loss 11.147683\n",
      "Epoch 2; Step 860; loss 13.632657\n",
      "Epoch 2; Step 865; loss 6.786166\n",
      "Epoch 2; Step 870; loss 13.312823\n",
      "Epoch 2; Step 875; loss 9.715940\n",
      "Epoch 2; Step 880; loss 10.986040\n",
      "Epoch 2; Step 885; loss 20.128090\n",
      "Epoch 2; Step 890; loss 13.031626\n",
      "Epoch 2; Step 895; loss 3.136998\n",
      "Epoch 2; Step 900; loss 4.283367\n",
      "Epoch 2; Step 905; loss 3.199272\n",
      "Epoch 2; Step 910; loss 14.924655\n",
      "Epoch 2; Step 915; loss 7.536666\n",
      "Epoch 2; Step 920; loss 8.625374\n",
      "Epoch 2; Step 925; loss 6.060975\n",
      "Epoch 2; Step 930; loss 23.226620\n",
      "Epoch 2; Step 935; loss 6.506720\n",
      "Epoch 2; Step 940; loss 7.209857\n",
      "Epoch 2; Step 945; loss 7.772868\n",
      "Epoch 2; Step 950; loss 10.790378\n",
      "Epoch 2; Step 955; loss 6.387761\n",
      "Epoch 2; Step 960; loss 8.231447\n",
      "Epoch 2; Step 965; loss 10.618425\n",
      "Epoch 2; Step 970; loss 10.698616\n",
      "Epoch 2; Step 975; loss 10.116301\n",
      "Epoch 2; Step 980; loss 6.467021\n",
      "Epoch 2; Step 985; loss 6.630694\n",
      "Epoch 2; Step 990; loss 25.364847\n",
      "Epoch 2; Step 995; loss 7.563278\n",
      "Epoch 2; Step 1000; loss 11.721748\n",
      "Epoch 2; Step 1005; loss 9.952472\n",
      "Epoch 2; Step 1010; loss 5.694697\n",
      "Epoch 2; Step 1015; loss 7.221321\n",
      "Epoch 2; Step 1020; loss 4.978277\n",
      "Epoch 2; Step 1025; loss 18.017994\n",
      "Epoch 2; Step 1030; loss 21.731928\n",
      "Epoch 2; Step 1035; loss 5.180280\n",
      "Epoch 2; Step 1038; Avg Loss 0.013535; Train loss: 9.265601; Dev loss: 14.858558; Dev kappa: 0.911426\n",
      "Epoch 3; Step 1040; loss 3.327174\n",
      "Epoch 3; Step 1045; loss 9.598179\n",
      "Epoch 3; Step 1050; loss 4.948233\n",
      "Epoch 3; Step 1055; loss 7.830108\n",
      "Epoch 3; Step 1060; loss 6.430347\n",
      "Epoch 3; Step 1065; loss 2.434042\n",
      "Epoch 3; Step 1070; loss 19.151196\n",
      "Epoch 3; Step 1075; loss 5.876655\n",
      "Epoch 3; Step 1080; loss 16.560619\n",
      "Epoch 3; Step 1085; loss 4.959715\n",
      "Epoch 3; Step 1090; loss 3.100089\n",
      "Epoch 3; Step 1095; loss 12.200930\n",
      "Epoch 3; Step 1100; loss 4.866434\n",
      "Epoch 3; Step 1105; loss 25.691349\n",
      "Epoch 3; Step 1110; loss 5.839026\n",
      "Epoch 3; Step 1115; loss 7.640531\n",
      "Epoch 3; Step 1120; loss 5.193912\n",
      "Epoch 3; Step 1125; loss 10.116964\n",
      "Epoch 3; Step 1130; loss 3.692397\n",
      "Epoch 3; Step 1135; loss 10.864936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8b9cb6deba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdev_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start training:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f3e7649b5ff0>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter, train_eval_iter)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mlossy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#######\n",
    "# Train\n",
    "\n",
    "# Hyper Parameters \n",
    "model = 'LSTM'\n",
    "input_size = vocab_size\n",
    "hidden_dim = 24\n",
    "embedding_dim = 100\n",
    "batch_size = 20\n",
    "learning_rate = 0.1\n",
    "num_epochs = 500\n",
    "num_layer = 1\n",
    "bi_direction = True\n",
    "\n",
    "# Build, initialize, and train model\n",
    "rnn = Model.LSTM(model, vocab_size, embedding_dim, hidden_dim, num_layer, dropout=0.2, bidirectional=bi_direction, \n",
    "pre_emb=None)\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set, batch_size)\n",
    "dev_iter = eval_iter(dev_set, batch_size)\n",
    "print('start training:')\n",
    "training_loop(batch_size, num_epochs, rnn, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.8558e-01  4.7335e-02 -1.3447e-01  ...   7.8784e-02 -5.5189e-03 -1.7380e-01\n",
       " 9.2983e-02 -3.6085e-02 -8.1732e-02  ...  -9.1744e-02  7.3796e-02 -1.5655e-02\n",
       " 7.5980e-02 -5.6447e-02  5.3446e-02  ...  -6.9504e-02  8.3970e-02 -7.4207e-02\n",
       "                ...                   ⋱                   ...                \n",
       "-6.5086e-02 -7.0979e-02 -5.5513e-02  ...  -7.6042e-02  9.2168e-02 -6.8419e-02\n",
       "-3.2056e-02 -6.2593e-02 -5.3247e-02  ...   5.5068e-02 -5.7677e-03  5.9561e-03\n",
       " 2.7188e-02 -7.6712e-02  7.0722e-02  ...  -9.8399e-02 -3.1293e-02  6.6518e-02\n",
       "[torch.FloatTensor of size 68573x100]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next(rnn.parameters()).data=rnn.encoder.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, labels = get_batch(next(training_iter)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, labels = get_batch(next(training_iter)) \n",
    "vectors = torch.stack(vectors).squeeze()\n",
    "vectors = vectors.transpose(1, 0)\n",
    "\n",
    "labels = Variable(torch.stack(labels).squeeze().type('torch.FloatTensor')) \n",
    "vectors = Variable(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 59350   6867  15880  ...   15880  39854  26002\n",
       " 61349  65071  30726  ...   68281  59350   6578\n",
       " 68553   6911  53084  ...   31959  19633  37555\n",
       "        ...            ⋱           ...         \n",
       "     0      0      0  ...       0      0      0\n",
       "     0      0      0  ...       0      0      0\n",
       "     0      0      0  ...       0      0      0\n",
       "[torch.LongTensor of size 1064x20]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
