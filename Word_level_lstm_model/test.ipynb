{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os \n",
    "import data\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import Model\n",
    "import random\n",
    "\n",
    "\n",
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    total_samples = total_batches * batch_size\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        vectors = vectors.transpose(1, 0)\n",
    "        \n",
    "        labels = Variable(torch.stack(labels).squeeze().type('torch.FloatTensor')) \n",
    "        vectors = Variable(vectors)\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(vectors, hidden)\n",
    "        lossy = loss_(output, labels)\n",
    "        epoch_loss += lossy.data[0] * batch_size\n",
    "\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            loss_train = evaluate(model, train_eval_iter,batch_size)\n",
    "            loss_dev = evaluate(model, dev_iter,batch_size)\n",
    "            kappa_dev = evaluate_kappa(model, dev_iter,batch_size)\n",
    "            with open(\"test.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"Epoch %i; Step %i; Avg Loss %f; Train loss: %f; Dev loss: %f; Dev kappa: %f\\n\" \n",
    "                  %(epoch, step, epoch_loss/total_samples, loss_train, loss_dev, kappa_dev))\n",
    "            print(\"Epoch %i; Step %i; Avg Loss %f; Train loss: %f; Dev loss: %f; Dev kappa: %f\" \n",
    "                  %(epoch, step, epoch_loss/total_samples, loss_train, loss_dev, kappa_dev))\n",
    "            epoch += 1\n",
    "            \n",
    "        if step % 5 == 0:\n",
    "            with open(\"test.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"Epoch %i; Step %i; loss %f\\n\" %(epoch, step, lossy.data[0]))\n",
    "            print(\"Epoch %i; Step %i; loss %f\" %(epoch, step, lossy.data[0]))\n",
    "        step += 1\n",
    "\n",
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter, batch_size):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    evalloss = 0.0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        vectors = vectors.transpose(1, 0)\n",
    "        \n",
    "        labels = Variable(torch.stack(labels).squeeze().type('torch.FloatTensor'))\n",
    "        vectors = Variable(vectors)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(vectors, hidden)\n",
    "        evalloss += F.mse_loss(output, labels).data[0]\n",
    "    return evalloss/len(data_iter)\n",
    "\n",
    "\n",
    "def evaluate_kappa(model, data_iter, batch_size):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        vectors = vectors.transpose(1, 0)\n",
    "\n",
    "        vectors = Variable(vectors)\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(vectors, hidden)\n",
    "\n",
    "        predicted = [int(round(float(num))) for num in output.data.cpu().numpy()]\n",
    "        predicted_labels.extend([round(float(num)) for num in output.data.cpu().numpy()])\n",
    "        labels = [int(label[0]) for label in labels]\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "    return cohen_kappa_score(true_labels, predicted_labels, weights = \"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0.2, bidirectional = False, pre_emb=None):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = getattr(nn, rnn_type)(embedding_dim, hidden_size, num_layers, bias=False, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.decoder = nn.Linear(hidden_size, 1)\n",
    "        self.decoder_bi = nn.Linear(hidden_size*2, 1)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.init_weights(pre_emb)\n",
    "        \n",
    "    def init_weights(self, pretrained_embedding):\n",
    "            initrange = 0.1\n",
    "            if(pretrained_embedding is not None):\n",
    "                pretrained_embedding = pretrained_embedding.astype(np.float32)\n",
    "                pretrained_embedding = torch.from_numpy(pretrained_embedding)\n",
    "                self.encoder.weight.data = pretrained_embedding\n",
    "            else:\n",
    "                self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "            self.decoder.bias.data.fill_(0)\n",
    "            self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.encoder(inputs)\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        # mot here\n",
    "        self.mot = torch.mean(output, 0)\n",
    "        self.mot = torch.squeeze(output)\n",
    "        print(self.mot.size())\n",
    "        if self.bidirectional:\n",
    "            decoded = self.decoder_bi(output)\n",
    "        else:\n",
    "            decoded = self.decoder(output)\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ##next(rnn.parameters()).data=rnn.encoder.weight.data\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.bidirectional == True:\n",
    "            return (Variable(weight.new(self.num_layers * 2, batch_size, self.hidden_size).zero_()),\n",
    "                    Variable(weight.new(self.num_layers * 2, batch_size, self.hidden_size).zero_()))\n",
    "        else:\n",
    "            return (Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_()),\n",
    "                    Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_()))            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading!\n",
      "max seq length:  1064\n",
      "data_size 12977\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"../data/training_final.csv\", sep=',',header=0, index_col=0)\n",
    "data_set = data.get_data(raw_data)\n",
    "print('Finished Loading!')\n",
    "\n",
    "#get max sequence length\n",
    "max_seq_length = max(list(map(lambda x:len(x.split()),raw_data.essay)))\n",
    "print('max seq length: ', max_seq_length)\n",
    "\n",
    "# split to train/val/test\n",
    "data_size = len(data_set)\n",
    "print('data_size',data_size)\n",
    "training_set = data_set[:int(data_size*0.8)]\n",
    "dev_set = data_set[int(data_size*0.8):int(data_size*0.9)]\n",
    "test_set = data_set[int(data_size*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set.to_csv('test_set.csv', index = False)\n",
    "pickle.dump(test_set,open('test_set.pk', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Converting!\n"
     ]
    }
   ],
   "source": [
    "# convert and formatting\n",
    "word_to_ix, index_to_word, vocab_size = data.build_dictionary([training_set])\n",
    "#print('vocab size', vocab_size)\n",
    "data.sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set], max_seq_length)\n",
    "print('Finished Converting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'essay_set': 6, 'label': \n",
       "  9\n",
       " [torch.LongTensor of size 1], 'text': 'in the excerpt from @organization2\\'s the mooring mast, the builders of the empire state building faced many obstacles in allowing dirigibles to dock there. for example, \"the lack of a suitable landing area.\" architects cannot just drop a mooring mast on top of the empire state building\\'s roof. the building would have had an extreme amount of pressure on it. in addition, along with the pressure, the dirigibles would \"add stress to the building\\'s frame.\" the builders needed to modify and strengthen the steel frame of the empire state building. this also cost a great deal of money. furthermore, \"the greatest reason was safety.\" the slightest mistake in building the mast, could affect every person in the building. in order to allow dirigibles to dock there, the builders needed to be aware of all the citizens surrounding the building. all in all, these obstacles determined the fate of the mast.', 'text_index_sequence': \n",
       "   9679  11275  10842  ...       0      0      0\n",
       " [torch.LongTensor of size 1x1064]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing glove dictionary to save time\n",
      "start training:\n",
      "Epoch 0; Step 0; Avg Loss 0.731220; Train loss: 114.863277; Dev loss: 110.861959; Dev kappa: -0.036373\n",
      "Epoch 1; Step 0; loss 75.315620\n",
      "Epoch 1; Step 5; loss 96.604324\n",
      "Epoch 1; Step 10; loss 85.804970\n",
      "Epoch 1; Step 15; loss 46.120274\n",
      "Epoch 1; Step 20; loss 106.849983\n",
      "Epoch 1; Step 25; loss 38.159824\n",
      "Epoch 1; Step 30; loss 72.105881\n",
      "Epoch 1; Step 35; loss 66.949478\n",
      "Epoch 1; Step 40; loss 37.453758\n",
      "Epoch 1; Step 45; loss 50.282639\n",
      "Epoch 1; Step 50; loss 40.439720\n",
      "Epoch 1; Step 55; loss 21.929771\n",
      "Epoch 1; Step 60; loss 15.146441\n",
      "Epoch 1; Step 65; loss 15.030107\n",
      "Epoch 1; Step 70; loss 24.175129\n",
      "Epoch 1; Step 75; loss 7.333912\n",
      "Epoch 1; Step 80; loss 14.395830\n",
      "Epoch 1; Step 85; loss 12.380017\n",
      "Epoch 1; Step 90; loss 6.457202\n",
      "Epoch 1; Step 95; loss 9.085093\n",
      "Epoch 1; Step 100; loss 8.120253\n",
      "Epoch 1; Step 103; Avg Loss 0.083561; Train loss: 9.765585; Dev loss: 11.773623; Dev kappa: 0.921521\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######\n",
    "# Train\n",
    "\n",
    "# Hyper Parameters \n",
    "model = 'LSTM'\n",
    "input_size = vocab_size\n",
    "hidden_dim = 24\n",
    "embedding_dim = 50\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1\n",
    "num_layer = 1\n",
    "bi_direction = True\n",
    "\n",
    "\n",
    "matrix = np.zeros((2, int(embedding_dim)))\n",
    "\n",
    "oov=0\n",
    "glove = {}\n",
    "filtered_glove = {}\n",
    "glove_path = '../data/filtered_glove_50.p'\n",
    "if(os.path.isfile(glove_path)):\n",
    "    print(\"Reusing glove dictionary to save time\")\n",
    "    pretrained_embedding = pickle.load(open(glove_path,'rb'))\n",
    "else:\n",
    "    #print('loading glove embedding')\n",
    "    with open('../data/glove.6B.50d.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            vec = l.split(' ')\n",
    "            glove[vec[0].lower()] = np.array(vec[1:])\n",
    "    print('glove size={}'.format(len(glove)))\n",
    "    print(\"Finished making glove dictionary\")\n",
    "\n",
    "    for i in range(2, len(index_to_word)):\n",
    "        word = index_to_word[i]\n",
    "        if(word in glove):\n",
    "            vec = glove[word]\n",
    "            filtered_glove[word] = glove[word]\n",
    "            matrix = np.vstack((matrix,vec))\n",
    "        else:\n",
    "            oov+=1\n",
    "            random_init = np.random.uniform(low=-0.01,high=0.01, size=(1,embedding_dim))\n",
    "            matrix = np.vstack((matrix,random_init))\n",
    "\n",
    "    pickle.dump(matrix, open(\"../data/filtered_glove_50.p\", \"wb\"))\n",
    "    #print(matrix.shape)\n",
    "    pretrained_embedding = matrix\n",
    "    #print(\"word_to_ix\", len(word_to_ix))\n",
    "    #print(\"oov={}\".format(oov))\n",
    "    #print(\"Saving glove vectors\")\n",
    "    print(\"Saving glove vectors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build, initialize, and train model\n",
    "rnn = Model.LSTM(model, vocab_size, embedding_dim, hidden_dim, num_layer, dropout=0.2, bidirectional=bi_direction, \n",
    "pre_emb=None)\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set, batch_size)\n",
    "dev_iter = eval_iter(dev_set, batch_size)\n",
    "print('start training:')\n",
    "training_loop(batch_size, num_epochs, rnn, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_iter = eval_iter(dev_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': \n",
       "  6\n",
       " [torch.LongTensor of size 1],\n",
       " 'text': \"In this world you never know what someone might writing. There're many materials in this world that I think aren't appropiate for someone to be reading, watching or listenning, ect.  You @MONTH1 be thinking that anyone makes his/her own decision of what they read, watch, listen.  Well, I do agree with that, but I do believe that some materials aren't rate it properly to age.  I do think that some materials should be banned from people under age, some from kids, and some from everyone.      You @MONTH1 be thinking that all of this is wrong and that there is nothing wrong with books, music, movies, ect., but in my opinion some materials are just offensive and not proper. I seen so many materials that I think are not proper for people under age to been looking at. In some movies, for example, the rating is just not right because some movies have so much violence or language.  You might not think it's that big of a deal, of course, but I do. In my opinion, some materials are not too low in rating, and should not be for people under age.     It's not surprising to me that kids are now yelling, hitting, and or acting like a rebels to their parents. Many of this things kids get it from books, music, movies, ect. This is many of the reasons some materials should be banned from kids.  You ask yourself why? Well, first of all, kids are just learnning and they do everything they watch, see, or listen. I mean now kids don't even think about what their about to do.  They just do, they don't care, they think it's okay. I think that parents should be alert to offensive materials in the shelves, and should say something about it so they banned them.     Why banned some materials from everyone?  Well, first of all, why do we even want to watch, see, or listen to things that are not right? You @MONTH1 be thinking that we just do it for fun and that it's enjoyable and nothing wrong with it. Well, you @MONTH1 be right but also wrong.  I certainly do agree with you, we all watched things for fun and to enjoy them, but some things are just too out of this world for us. Why do people do crazy things now in these day? In my opinion, I think is because they mostly get it from books, music, movies, ect.  I think that if a book shows very offensive things they should be banned from everyone no matter what. It's the right thing to do.     If you don't want to see this world get more out of control than it already is. Well, we need to do something about all these materials that are offensive and banned them for good.  We certainly don't want to see our kids yelling, hitting, or and actting like rebels to us. We don't want some crazy person coming to our hosue and do crazy things to us just because that's what they learn from those materials they had on the shelves.  It's come to me that some materials should be banned from people under age, from kids, and some from everyone\",\n",
       " 'text_index_sequence': \n",
       "   2892  17262  12422  ...       0      0      0\n",
       " [torch.LongTensor of size 1x1064]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(next(training_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
